下面给你一个**从 0 到 1 可落地**的端到端流程（含数据结构、关键脚本拆分、实现要点与坑点）。我默认你最终要生成两个产物：

1. **Adressa/news.tsv**：增加一列（或替换一列）`title_entities`，其格式**尽量对齐 MIND**：每条新闻的标题实体是一个 JSON list，每个实体包含 `Label/Type/WikidataId/Confidence/OccurrenceOffsets/SurfaceForms`（MIND 官方示例就是这种结构） ([Microsoft Learn][1])
2. **entity_embedding.vec（挪威语版）**：与 MIND 的 `entity_embedding.vec` 同格式：每行 `Qxxx` + 100 维 float；这些 embedding 在 MIND 中是从 Wikidata 子图用 TransE 学到的 100 维向量 ([Microsoft Learn][1])

---

## 0. 先把目标“规格”钉死

### 0.1 news.tsv 里 title_entities 的目标格式（对齐 MIND）

MIND 的 `news.tsv` 有列：`title_entities`、`abstract_entities`，其中 entities 列内容是类似这样的 JSON：
`[{“Label”: “…”, “Type”: “…”, “WikidataId”: “Q…”, “Confidence”: 1.0, “OccurrenceOffsets”: [..], “SurfaceForms”: [..]}]` ([Microsoft Learn][1])

你这次只做标题，所以可以：

* 只写 `title_entities`
* `abstract_entities` 留空 `[]` 或原样不动（取决于你现有 Adressa/news.tsv 结构）

### 0.2 entity_embedding.vec 的目标格式（对齐 MIND）

* 每行：`<WikidataId> <f1> <f2> ... <f100>`
* 维度固定 100（因为你要用 MIND 的向量做初始化，MIND 就是 100 维） ([Microsoft Learn][1])

---

## 1. 数据准备：把 Adressa 的“新闻侧”整理到可处理形态

Adressa 原始数据常见是 JSON（文档里也明确说明数据以 JSON 存储，且 light version 里就包含 `title`、`namedEntities` 等字段） 
所以你会遇到两种情况：

### 情况 A：你已经有 MIND 风格的 Adressa/news.tsv

直接跳到第 2 节（实体抽取）。

### 情况 B：你只有 Adressa 原始 JSON

你需要先做一个**构造版 news.tsv**（最少包含：`news_id/title/url/category/...`）。
建议字段设计（MIND 对齐的最小可用版）：

* `news_id`：用 `documentId` 或 canonicalUrl hash
* `category/subcategory`：Adressa 有 `category` 字段（light version） 
* `title`：Adressa 的 `title` 
* `abstract/url`：没有就空字符串
* `title_entities`：先置空 `[]`，后面回填
* `abstract_entities`：`[]`

---

## 2. 实体抽取：用 NbBERT 在挪威语标题做 NER

### 2.1 模型选择

直接用现成的 NER 微调模型：`NbAiLab/nb-bert-base-ner`（它是 NB-BERT base 在 NorNE NER 上 fine-tune 的） ([Hugging Face][2])
这一步的目标不是“链接”，只是抽出 mention span（文本片段）+ 粗粒度类型（PER/ORG/LOC/MISC）。

### 2.2 实现要点

**建议用 HuggingFace token-classification pipeline**，并开启 span 聚合（把子词拼成实体）：

* 输入：title 字符串
* 输出：若干实体 mention，带 `start/end`（字符偏移）、`word`（surface form）、`entity_group`（类型）、`score`（NER 置信度）

你要保留：

* `surface_form = title[start:end]`
* `offset = start`（OccurrenceOffsets 用字符级起始位置，跟 MIND 一致） ([Microsoft Learn][1])
* `ner_conf = score`

### 2.3 清洗规则（强烈建议）

否则你后续 Wikidata 会炸：

* 去掉纯数字、单字符、全标点
* 合并重复 mention（同 surface、同 offset）
* 对同一标题里重复出现的 mention：保留所有 offset（MIND 的 `OccurrenceOffsets` 就是 list） ([Microsoft Learn][1])

---

## 3. 实体链接：从 mention → WikidataId（QID）

这是整条链路里最容易“变成研究课题”的部分，所以我给你一个**工程可落地**的两阶段链接方案：**候选召回 → 候选排序/消歧**。

### 3.1 候选召回（Candidate Generation）

对每个 mention，用 Wikidata 的搜索接口召回 topK（比如 K=10）候选 QID：

* 优先语言：`no` / `nb`（挪威语 Bokmål），召回更贴近标题原文
* 召回内容：QID、label、description（后面用于排序）

（Wikidata 的数据访问/接口建议遵守 maxlag/礼仪等要求） ([维基数据][3])

> 工程建议：对 `surface_form` 做缓存（SQLite/LMDB/JSON 都行），否则你会被速率限制折磨。

### 3.2 候选排序（Ranking / Disambiguation）

给每个候选打分，选最高的作为链接结果。一个实用的打分函数：

**总分 = 词面匹配 + 语义相似 + 类型一致 + 本地语言偏好**

1. **词面匹配（string match）**

* mention 与候选 label 完全一致：+a
* mention 与候选 alias/redirect 近似（大小写、去标点、编辑距离）：+b

2. **语义相似（context match）**

* 用一个**多语句向量模型**（如 multilingual SBERT）算：

  * `sim(title, candidate_description)`
  * 或 `sim(title, candidateUS: candidate_label + candidate_description)`
* 这一步能显著降低 “同名不同人/队伍/地名” 的误链

3. **类型一致（type constraint）**

* NER 的 `PER/ORG/LOC` 可映射到 Wikidata 的 `instance of (P31)` 大类：

  * PER → human(Q5)
  * ORG → organization/company/sports team 等
  * LOC → geographic entity
* 对 top2 候选再调用一次实体详情（取 P31）做过滤/加分

> 注意：MIND 的 `Type` 字段是 “Wikidata 中的类型”，但它具体编码方式（O/P/L…）不一定与你 NER 一致；工程上你可以先写入 NER 粗类型，或写入你根据 P31 归一后的类型。这里我不能保证与你期望的 Type 完全一致，只能保证**结构可用**。

4. **挪威语 sitelink 偏好（可选但很有用）**
   如果候选有挪威语维基百科 sitelink（`nowiki`/`nbwiki`），加分；这通常更符合挪威语新闻标题语境。

### 3.3 置信度（Confidence）怎么给

你可以用一个可解释的组合：

* `confidence = ner_conf * softmax(rank_score_top1 vs topk)`
  或者简单点：
* `confidence = ner_conf * normalized(rank_score_top1)`

---

## 4. 回填 Adressa/news.tsv：生成 title_entities JSON

对每条新闻 title，整理成 MIND 风格 JSON list： ([Microsoft Learn][1])

每个实体对象建议字段：

* `Label`：候选实体的 Wikidata label（优先英文或挪威语都行，但建议**用 Wikidata 返回的标准 label**）
* `Type`：你归一后的类型（或 NER 类型）
* `WikidataId`：QID（如 Q910409） ([Microsoft Learn][1])
* `Confidence`：上节算的 confidence
* `OccurrenceOffsets`：该实体在 title 中出现的所有起始字符位置 list
* `SurfaceForms`：对应每个 offset 的原始 surface form list（若你做了归一化，这里仍应存原文片段）

**去重策略（推荐）**：同一 title 内按 `WikidataId` 聚合，把 offsets/surfaceForms 合并。

---

## 5. 从 WikidataId 对齐到 MIND 的英文实体 embedding

### 5.1 读入 MIND 的 entity_embedding.vec

MIND 的 `entity_embedding.vec`：

* 第一列是实体/关系 ID（实体就是 QID）
* 后面是 100 维向量（TransE 学的） ([Microsoft Learn][1])
  且官方提醒：**有些实体可能缺 embedding** ([Microsoft Learn][1])

### 5.2 构建你的 Adressa 实体词表

从你生成的 `news.tsv/title_entities` 抽出所有 QID：

* `E = {QID_1, QID_2, ...}`

### 5.3 初始化向量表 `E → R^100`

对每个 QID：

* 若在 MIND `entity_embedding.vec` 中存在：`init_vec = mind_vec[QID]`
* 否则：随机初始化（例如 N(0, 0.01)），并打一个 mask 标记 `is_pretrained=0`

> 注意：虽然你称为“英文 embedding”，但本质上它们是 Wikidata 子图的 TransE embedding，语言无关；只是来源于 MIND（英文新闻） ([Microsoft Learn][1])

---

## 6. 训练“挪威语实体嵌入表”：推荐两条可落地路线

你要的是“用英文 embedding 做初始化，训练出挪威语域内更适配的实体 embedding”。我给你两条路线，你可以按你研究目标选；**最终都会产出同格式 entity_embedding.vec**。

---

### 路线 A（最直接、最像你描述）：用挪威语标题上下文对实体向量做对齐微调（推荐）

**核心思想**：
同一个实体 QID 在挪威语新闻里的上下文（NbBERT 表示）应该接近该实体 embedding；用对比学习把实体向量往“挪威语语境”拉。

**训练数据**：每个 (title, entity mention span, QID)

**模型组件**

* NbBERT 编码 title，取 mention span 的 contextual embedding：`m ∈ R^768`
* 一个投影层：`W: R^768 → R^100` 得到 `m'`
* 可训练实体表：`E_emb[QID] ∈ R^100`（初始化来自 MIND）

**损失（InfoNCE）**
对每个正样本 QID：

* 正样本相似度：`sim(m', E_emb[q+])`
* 负样本：同 batch 其他 QID，或从全体实体里采样 N 个
* softmax / 对比损失让正样本最大

**优点**

* 真正把 NbBERT 的挪威语语义注入 entity embedding
* 不依赖你拿到 Wikidata 的大规模三元组

**注意点**

* 你必须确保 entity linking 的准确度别太差，否则会“学歪”

---

### 路线 B（更像 MIND 的来源）：在 Wikidata 子图上继续训练 TransE，但只微调你需要的实体（可选）

MIND 的 embedding 来自 Wikidata 子图 TransE ([Microsoft Learn][1])。
你可以抓取包含 `E` 的子图三元组 `(h, r, t)`，继续训练 TransE/RotatE：

* 实体向量初始化：MIND
* 关系向量：可用 MIND 的 relation_embedding.vec 初始化（若你也引入）
* 训练：margin ranking loss

**但它“语言注入”很弱**（更偏 KG 结构），适合你想要“更干净的 KG embedding 延续”。

---

## 7. 导出最终 entity_embedding.vec（对齐 MIND）

训练完后，你把 `E_emb` 写回文件：

* 每行：`QID` + 100 floats（空格分隔）
* 不写 header（MIND 示例也是每行直接写） ([Microsoft Learn][1])

---

## 8. 推荐的脚本拆分（你可以直接照这个工程结构搭）

```
project/
  data/
    adressa/news.tsv
    mind/entity_embedding.vec
  cache/
    wikidata_search.sqlite
    qid_details.sqlite
  scripts/
    00_build_adressa_news_tsv.py        # 若你只有 JSON，则先构造 news.tsv
    01_ner_titles_nbbert.py             # NbBERT NER 抽 mention（输出中间结果）
    02_link_to_wikidata.py              # mention -> QID（含缓存）
    03_write_title_entities_to_tsv.py   # 回填 news.tsv 的 title_entities
    04_build_entity_vocab_and_init.py   # 从 news.tsv 抽 QID + 用 MIND 初始化
    05_train_entity_embeddings_no.py    # 路线A 或 路线B
    06_export_entity_embedding_vec.py   # 导出最终 entity_embedding.vec
```

每个脚本都应该产出**可检查的中间文件**（CSV/JSONL），比如：

* `ner_mentions.jsonl`：每条新闻的 mention spans
* `linked_entities.jsonl`：每条 mention 的 topK 候选与最终选中 QID
* `entity_init_stats.txt`：多少 QID 命中 MIND，多少随机初始化（因为 MIND 也可能缺 embedding） ([Microsoft Learn][1])

---

## 9. 质量与正确性检查（不做这步，后面训练很可能白跑）

### 9.1 链接质量抽检

随机抽 200 条 title：

* 打印 title、surface form、选中的 QID label/desc
* 人工估计 accuracy（哪怕粗略也能判断是否可用）

### 9.2 覆盖率

* `#unique_QID_in_adressa`
* `#QID_hit_in_MIND_embedding`
* `coverage = hit / total`

coverage 太低（比如 <30%）时：

* 你的“用英文 embedding 初始化”的收益会变小
* 需要考虑：对未命中实体用路线 A 的训练更依赖数据量；或额外引入 KG 子图训练

### 9.3 news.tsv 写回合法性

* `title_entities` 必须是合法 JSON（否则后续 dataloader 直接崩）
* `OccurrenceOffsets` 必须是字符级 int list，并且 offset 能在 title 中定位到 surface form（这一步能抓出 tokenizer offset 的 bug） ([Microsoft Learn][1])

---

## 10. 你最可能踩的坑（提前规避）

1. **NER 的 offset 与原字符串不一致**
   要用 fast tokenizer 的 offset_mapping 或 pipeline 自带的 start/end；不要自己用 token index 猜字符位置。

2. **Wikidata 同名实体极多**
   必须做“候选排序”，只用 top1 搜索结果会大量误链。

3. **标题太短导致消歧困难**
   可以把 category/section（如果有）拼到 title 后当上下文，例如：`"<category> [SEP] <title>"` 再做相似度。

4. **速率限制**
   必须缓存；并遵守 maxlag 等建议 ([维基数据][3])

---